{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Recurrent Neural Networks(1).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGXc5iMcD/x+CSvMbDObgA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9fNvK-J-Q7J"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==0.14.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.5 Implementation of Recurrent Neural Networks from Scratch**"
      ],
      "metadata": {
        "id": "BYDyt7BT-Xde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "bFDYaQSQ-lMc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcPiXqKJ-soT",
        "outputId": "8fbd561a-a4fc-441c-d3b3-bda736e05c10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.1 One-Hot Encoding"
      ],
      "metadata": {
        "id": "uq3oE8sY-4LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(torch.tensor([0, 2]), len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFXaqVGD-z2W",
        "outputId": "15eaf4df-1a88-4a97-b8cd-e5123b4931fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.arange(10).reshape((2, 5))\n",
        "F.one_hot(X.T, 28).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMJUlN4t_iSp",
        "outputId": "9d370b55-39af-4801-f32d-8b6cb381f9f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.2 Initializing the Model Parameters"
      ],
      "metadata": {
        "id": "Lh4w0zio_rs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "  num_inputs = num_outputs = vocab_size\n",
        "\n",
        "  def normal(shape):\n",
        "    return torch.randn(size=shape, device=device) * 0.01\n",
        "\n",
        "    # Hidden layer parameters\n",
        "    W_xh = normal((num_inputs, num_hiddens))\n",
        "    W_hh = normal((num_hiddens, num_hiddens))\n",
        "    b_h = torch.zeros(num_hiddens, device=device)\n",
        "    # Attach gradients\n",
        "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "    for param in params:\n",
        "      param.requires_grad_(True)\n",
        "    return params"
      ],
      "metadata": {
        "id": "Ts4_msLI_vMP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.3 RNN Model"
      ],
      "metadata": {
        "id": "ir1253xOCd7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "  return (torch.zeros((batch_size, num_hiddens), device=device), )"
      ],
      "metadata": {
        "id": "rfo261BfAtc7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(inputs, state, params):\n",
        "  # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "  W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "  H, = state\n",
        "  outputs = []\n",
        "  # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "  for X in inputs:\n",
        "      H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
        "      Y = torch.mm(H, W_hq) + b_q\n",
        "      outputs.append(Y)\n",
        "  return torch.cat(outputs, dim=0), (H,)"
      ],
      "metadata": {
        "id": "s4i8WJtQA7L3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModelScratch: #@save\n",
        "  \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
        "  def __init__(self, vocab_size, num_hiddens, device,\n",
        "              get_params, init_state, forward_fn):\n",
        "      self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "      self.params = get_params(vocab_size, num_hiddens, device)\n",
        "      self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "  def __call__(self, X, state):\n",
        "      X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "      return self.forward_fn(X, state, self.params)\n",
        "\n",
        "  def begin_state(self, batch_size, device):\n",
        "      return self.init_state(batch_size, self.num_hiddens, device)"
      ],
      "metadata": {
        "id": "dSTGDd8HCnZT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)\n",
        "state = net.begin_state(X.shape[0], d2l.try_gpu())"
      ],
      "metadata": {
        "id": "huZJPwkBDQAT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
        "Y.shape, len(new_state), new_state[0].shape"
      ],
      "metadata": {
        "id": "T1q4cF0yDlPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.4 Prediction"
      ],
      "metadata": {
        "id": "JFjjMxLUEnJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab, device): #@save\n",
        "  \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "  state = net.begin_state(batch_size=1, device=device)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "  for y in prefix[1:]: # Warm-up period\n",
        "      _, state = net(get_input(), state)\n",
        "      outputs.append(vocab[y])\n",
        "  for _ in range(num_preds): # Predict `num_preds` steps\n",
        "      y, state = net(get_input(), state)\n",
        "      outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "kiACNzT3EomP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "PQybUIRsJwk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.5 Gradient Clipping"
      ],
      "metadata": {
        "id": "bM4tRxj3J3zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(net, theta): #@save\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  if isinstance(net, nn.Module):\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "  else:\n",
        "    params = net.params\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "  if norm > theta:\n",
        "    for param in params:\n",
        "      param.grad[:] *= theta / norm"
      ],
      "metadata": {
        "id": "N4NajNbUJ4N8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5.6 Training"
      ],
      "metadata": {
        "id": "zqq7kjTkRvmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "  \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
        "  state, timer = None, d2l.Timer()\n",
        "  metric = d2l.Accumulator(2) # Sum of training loss, no. of tokens\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "    # Initialize `state` when either it is the first iteration or\n",
        "    # using random sampling\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "      # `state` is a tensor for `nn.GRU`\n",
        "        state.detach_()\n",
        "      else:\n",
        "      # `state` is a tuple of tensors for `nn.LSTM` and\n",
        "      # for our custom scratch implementation\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      # Since the `mean` function has been invoked\n",
        "      updater(batch_size=1)\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "hl2m6E00KEMo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "  \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
        "                        legend=['train'], xlim=[10, num_epochs])\n",
        "  # Initialize\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
        "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "  # Train and predict\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch_ch8(\n",
        "          net, train_iter, loss, updater, device, use_random_iter)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(predict('time traveller'))\n",
        "        animator.add(epoch + 1, [ppl])\n",
        "  print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))"
      ],
      "metadata": {
        "id": "7nsHEMVASQj9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "JK9gvdYDSorB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
        "init_rnn_state, rnn)\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
        "use_random_iter=True)"
      ],
      "metadata": {
        "id": "7oFl9LBgSubF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "• We can train an RNN-based character-level language model to generate text following the\n",
        "user-provided text prefix.\n",
        "\n",
        "• A simple RNN language model consists of input encoding, RNN modeling, and output generation.\n",
        "\n",
        "• RNN models need state initialization for training, though random sampling and sequential\n",
        "partitioning use different ways.\n",
        "\n",
        "• When using sequential partitioning, we need to detach the gradient to reduce computational\n",
        "cost.\n",
        "\n",
        "• A warm-up period allows a model to update itself (e.g., obtain a better hidden state than its\n",
        "initialized value) before making any prediction.\n",
        "\n",
        "• Gradient clipping prevents gradient explosion, but it cannot fix vanishing gradients."
      ],
      "metadata": {
        "id": "iaTNi1A4S3oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.6 Concise Implementation of Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "RNK-mxtqS77y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "xmiiAAkyS7eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.6.1 Defining the Model"
      ],
      "metadata": {
        "id": "HpBseMlhTB9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
      ],
      "metadata": {
        "id": "ANYyb7f-TAc3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.zeros((1, batch_size, num_hiddens))\n",
        "state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsY-cRbbTD6k",
        "outputId": "aaa4e369-337c-407c-e4f4-fdb8f78cde5a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "Y.shape, state_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpQfPlatTGkM",
        "outputId": "532053be-516c-4903-c2f2-f9e0c169af46"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "class RNNModel(nn.Module):\n",
        "  \"\"\"The RNN model.\"\"\"\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNNModel, self).__init__(**kwargs)\n",
        "    self.rnn = rnn_layer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_hiddens = self.rnn.hidden_size\n",
        "    # If the RNN is bidirectional (to be introduced later),\n",
        "    # `num_directions` should be 2, else it should be 1.\n",
        "    if not self.rnn.bidirectional:\n",
        "      self.num_directions = 1\n",
        "      self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "    else:\n",
        "      self.num_directions = 2\n",
        "      self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "    X = X.to(torch.float32)\n",
        "    Y, state = self.rnn(X, state)\n",
        "    # The fully connected layer will first change the shape of `Y` to\n",
        "    # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
        "    # (`num_steps` * `batch_size`, `vocab_size`).\n",
        "    output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "    return output, state\n",
        "\n",
        "  def begin_state(self, device, batch_size=1):\n",
        "    if not isinstance(self.rnn, nn.LSTM):\n",
        "    # `nn.GRU` takes a tensor as hidden state\n",
        "      return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                        batch_size, self.num_hiddens),\n",
        "                        device=device)\n",
        "    else:\n",
        "  # `nn.LSTM` takes a tuple of hidden states\n",
        "      return (torch.zeros((\n",
        "        self.num_directions * self.rnn.num_layers,\n",
        "        batch_size, self.num_hiddens), device=device),\n",
        "        torch.zeros((\n",
        "            self.num_directions * self.rnn.num_layers,\n",
        "            batch_size, self.num_hiddens), device=device))\n"
      ],
      "metadata": {
        "id": "d8cDcpG2hp0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.6.2 Training and Predicting"
      ],
      "metadata": {
        "id": "zE9wxJdpiX8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = d2l.try_gpu()\n",
        "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "net = net.to(device)\n",
        "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
      ],
      "metadata": {
        "id": "Ek1mhoVliYgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
      ],
      "metadata": {
        "id": "-m1st9z2idFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.7 Backpropagation Through Time**\n"
      ],
      "metadata": {
        "id": "TKwyalXIiiEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.7.1 Analysis of Gradients in RNNs"
      ],
      "metadata": {
        "id": "KXfLJmeRil2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.7.2 Backpropagation Through Time in Detail"
      ],
      "metadata": {
        "id": "pXAevNY0ivMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "• Backpropagation through time is merely an application of backpropagation to sequence\n",
        "models with a hidden state.\n",
        "\n",
        "• Truncation is needed for computational convenience and numerical stability, such as regular truncation and randomized truncation.\n",
        "\n",
        "• High powers of matrices can lead to divergent or vanishing eigenvalues. This manifests itself\n",
        "in the form of exploding or vanishing gradients.\n",
        "\n",
        "• For efficient computation, intermediate values are cached during backpropagation through\n",
        "time."
      ],
      "metadata": {
        "id": "K9TQMfZSizVR"
      }
    }
  ]
}