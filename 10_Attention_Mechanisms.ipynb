{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10. Attention Mechanisms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEFSPIik1Wb78OkJUt1mfu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_VyI4UhXXui"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==0.14.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10.1 Attention Cues**\n",
        "\n",
        "10.1.1 Attention Cues in Biology\n",
        "\n",
        "10.1.2 Queries, Keys, and Values\n",
        "\n",
        "10.1.3 Visualization of Attention"
      ],
      "metadata": {
        "id": "ZxaoMhO8r-uG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPQECO8fzcYe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n",
        "                  cmap='Reds'):\n",
        "    \"\"\"Show heatmaps of matrices.\"\"\"\n",
        "    d2l.use_svg_display()\n",
        "    num_rows, num_cols = matrices.shape[0], matrices.shape[1]\n",
        "    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\n",
        "    sharex=True, sharey=True, squeeze=False)\n",
        "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
        "      for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
        "        pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\n",
        "        if i == num_rows - 1:\n",
        "          ax.set_xlabel(xlabel)\n",
        "        if j == 0:\n",
        "          ax.set_ylabel(ylabel)\n",
        "        if titles:\n",
        "          ax.set_title(titles[j])\n",
        "    fig.colorbar(pcm, ax=axes, shrink=0.6);"
      ],
      "metadata": {
        "id": "iPVBI6H8sJMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = torch.eye(10).reshape((1, 1, 10, 10))\n",
        "show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
      ],
      "metadata": {
        "id": "TWnus53Qsbmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10.2 Attention Pooling: Nadaraya-Watson Kernel Regression**"
      ],
      "metadata": {
        "id": "M1As6R6msdky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "bEjgYRu_seuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2.1 Generating the Dataset"
      ],
      "metadata": {
        "id": "0K8V-v2EsgaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 50 # No. of training examples\n",
        "x_train, _ = torch.sort(torch.rand(n_train) * 5) # Training inputs"
      ],
      "metadata": {
        "id": "31zVYJxEsiJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 2 * torch.sin(x) + x**0.8\n",
        "  \n",
        "y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,)) # Training outputs\n",
        "x_test = torch.arange(0, 5, 0.1) # Testing examples\n",
        "y_truth = f(x_test) # Ground-truth outputs for the testing examples\n",
        "n_test = len(x_test) # No. of testing examples\n",
        "n_test"
      ],
      "metadata": {
        "id": "8BVP51wOslqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_kernel_reg(y_hat):\n",
        "  d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'],\n",
        "          xlim=[0, 5], ylim=[-1, 5])\n",
        "  d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);"
      ],
      "metadata": {
        "id": "m3cwli-DsuxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2.2 Average Pooling"
      ],
      "metadata": {
        "id": "hMYlb0TFsx4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = torch.repeat_interleave(y_train.mean(), n_test)\n",
        "plot_kernel_reg(y_hat)"
      ],
      "metadata": {
        "id": "P7Gj3riHsylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2.3 Nonparametric Attention Pooling"
      ],
      "metadata": {
        "id": "tx_hUNuss1Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of `X_repeat`: (`n_test`, `n_train`), where each row contains the\n",
        "# same testing inputs (i.e., same queries)\n",
        "X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))\n",
        "# Note that `x_train` contains the keys. Shape of `attention_weights`:\n",
        "# (`n_test`, `n_train`), where each row contains attention weights to be\n",
        "# assigned among the values (`y_train`) given each query\n",
        "attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)\n",
        "# Each element of `y_hat` is weighted average of values, where weights are\n",
        "# attention weights\n",
        "y_hat = torch.matmul(attention_weights, y_train)\n",
        "plot_kernel_reg(y_hat)"
      ],
      "metadata": {
        "id": "V25193K9sz_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),\n",
        "                  xlabel='Sorted training inputs',\n",
        "                  ylabel='Sorted testing inputs')"
      ],
      "metadata": {
        "id": "RMZGk8RYs3-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2.4 Parametric Attention Pooling"
      ],
      "metadata": {
        "id": "xo21yn8qs-Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones((2, 1, 4))\n",
        "Y = torch.ones((2, 4, 6))\n",
        "torch.bmm(X, Y).shape"
      ],
      "metadata": {
        "id": "M1DgSJ6Hs-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones((2, 10)) * 0.1\n",
        "values = torch.arange(20.0).reshape((2, 10))\n",
        "torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))"
      ],
      "metadata": {
        "id": "V-dmeE1ftDAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NWKernelRegression(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.w = nn.Parameter(torch.rand((1,), requires_grad=True))\n",
        "\n",
        "  def forward(self, queries, keys, values):\n",
        "    # Shape of the output `queries` and `attention_weights`:\n",
        "    # (no. of queries, no. of key-value pairs)\n",
        "    queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))\n",
        "    self.attention_weights = nn.functional.softmax(\n",
        "        -((queries - keys) * self.w)**2 / 2, dim=1)\n",
        "    # Shape of `values`: (no. of queries, no. of key-value pairs)\n",
        "    return torch.bmm(self.attention_weights.unsqueeze(1),\n",
        "            values.unsqueeze(-1)).reshape(-1)"
      ],
      "metadata": {
        "id": "IFjWKJPHtFZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of `X_tile`: (`n_train`, `n_train`), where each column contains the\n",
        "# same training inputs\n",
        "X_tile = x_train.repeat((n_train, 1))\n",
        "# Shape of `Y_tile`: (`n_train`, `n_train`), where each column contains the\n",
        "# same training outputs\n",
        "Y_tile = y_train.repeat((n_train, 1))\n",
        "# Shape of `keys`: ('n_train', 'n_train' - 1)\n",
        "keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))\n",
        "# Shape of `values`: ('n_train', 'n_train' - 1)\n",
        "values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))"
      ],
      "metadata": {
        "id": "VSVk_oLOtPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NWKernelRegression()\n",
        "loss = nn.MSELoss(reduction='none')\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
        "animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])\n",
        "for epoch in range(5):\n",
        "  trainer.zero_grad()\n",
        "  l = loss(net(x_train, keys, values), y_train)\n",
        "  l.sum().backward()\n",
        "  trainer.step()\n",
        "  print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')\n",
        "  animator.add(epoch + 1, float(l.sum()))"
      ],
      "metadata": {
        "id": "DFuWonZvtR6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of `keys`: (`n_test`, `n_train`), where each column contains the same\n",
        "# training inputs (i.e., same keys)\n",
        "keys = x_train.repeat((n_test, 1))\n",
        "# Shape of `value`: (`n_test`, `n_train`)\n",
        "values = y_train.repeat((n_test, 1))\n",
        "y_hat = net(x_test, keys, values).unsqueeze(1).detach()\n",
        "plot_kernel_reg(y_hat)"
      ],
      "metadata": {
        "id": "An6XEczntXW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),\n",
        "      xlabel='Sorted training inputs',\n",
        "      ylabel='Sorted testing inputs')"
      ],
      "metadata": {
        "id": "a9k7mgRBtak6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10.3 Attention Scoring Functions**"
      ],
      "metadata": {
        "id": "J6kScWTTtBHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "DpoQqEEAtfh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.3.1 Masked Softmax Operation"
      ],
      "metadata": {
        "id": "7xCegVzithQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "def masked_softmax(X, valid_lens):\n",
        "  \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "  # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n",
        "  if valid_lens is None:\n",
        "    return nn.functional.softmax(X, dim=-1)\n",
        "  else:\n",
        "    shape = X.shape\n",
        "    if valid_lens.dim() == 1:\n",
        "      valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "    else:\n",
        "      valid_lens = valid_lens.reshape(-1)\n",
        "    # On the last axis, replace masked elements with a very large negative\n",
        "    # value, whose exponentiation outputs 0\n",
        "    X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
        "                    value=-1e6)\n",
        "    return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "metadata": {
        "id": "J9-I_jSctidi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ],
      "metadata": {
        "id": "3bLPLRQ_tvHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
      ],
      "metadata": {
        "id": "ngkao0F9tvcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.3.2 Additive Attention"
      ],
      "metadata": {
        "id": "tkTtZQRbtxzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "class AdditiveAttention(nn.Module):\n",
        "  \"\"\"Additive attention.\"\"\"\n",
        "  def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
        "    super(AdditiveAttention, self).__init__(**kwargs)\n",
        "    self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
        "    self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
        "    self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, queries, keys, values, valid_lens):\n",
        "    queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "    # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n",
        "    # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n",
        "    # no. of key-value pairs, `num_hiddens`). Sum them up with\n",
        "    # broadcasting\n",
        "    features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "    features = torch.tanh(features)\n",
        "    # There is only one output of `self.w_v`, so we remove the last\n",
        "    # one-dimensional entry from the shape. Shape of `scores`:\n",
        "    # (`batch_size`, no. of queries, no. of key-value pairs)\n",
        "    scores = self.w_v(features).squeeze(-1)\n",
        "    self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
        "    # dimension)\n",
        "    return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "ENNOCigWtyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
        "# The two value matrices in the `values` minibatch are identical\n",
        "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
        "        2, 1, 1)\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
        "        dropout=0.1)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)"
      ],
      "metadata": {
        "id": "hJP5DDNF1DNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "                  xlabel='Keys', ylabel='Queries')"
      ],
      "metadata": {
        "id": "06rNcwgQ1LJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.3.3 Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "fuqhIZcf1QWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "class DotProductAttention(nn.Module):\n",
        "  \"\"\"Scaled dot product attention.\"\"\"\n",
        "  def __init__(self, dropout, **kwargs):\n",
        "    super(DotProductAttention, self).__init__(**kwargs)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
        "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
        "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
        "    # dimension)\n",
        "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
        "  def forward(self, queries, keys, values, valid_lens=None):\n",
        "    d = queries.shape[-1]\n",
        "    # Set `transpose_b=True` to swap the last two dimensions of `keys`\n",
        "    scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
        "    self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "    return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "ZrZovMvt1MgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "attention = DotProductAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)"
      ],
      "metadata": {
        "id": "CGPsKnPO1c_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "        xlabel='Keys', ylabel='Queries')"
      ],
      "metadata": {
        "id": "8E3Eshdy1gPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10.4 Bahdanau Attention**"
      ],
      "metadata": {
        "id": "VxRIVa2N1okq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.4.1 Model\n",
        "\n",
        "10.4.2 Defining the Decoder with Attention"
      ],
      "metadata": {
        "id": "d51_BcRE1r4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@save\n",
        "class AttentionDecoder(d2l.Decoder):\n",
        "  \"\"\"The base attention-based decoder interface.\"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    super(AttentionDecoder, self).__init__(**kwargs)\n",
        "  \n",
        "  @property\n",
        "  def attention_weights(self):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "NC8ioBfV1p4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "            dropout=0, **kwargs):\n",
        "      super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
        "      self.attention = d2l.AdditiveAttention(\n",
        "      num_hiddens, num_hiddens, num_hiddens, dropout)\n",
        "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "      self.rnn = nn.GRU(\n",
        "          embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "          dropout=dropout)\n",
        "      self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "\n",
        "  def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
        "    # Shape of `outputs`: (`num_steps`, `batch_size`, `num_hiddens`).\n",
        "    # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
        "    # `num_hiddens`)\n",
        "    outputs, hidden_state = enc_outputs\n",
        "    return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\n",
        "    # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
        "    # `num_hiddens`)\n",
        "    enc_outputs, hidden_state, enc_valid_lens = state\n",
        "    # Shape of the output `X`: (`num_steps`, `batch_size`, `embed_size`)\n",
        "    X = self.embedding(X).permute(1, 0, 2)\n",
        "    outputs, self._attention_weights = [], []\n",
        "    for x in X:\n",
        "      # Shape of `query`: (`batch_size`, 1, `num_hiddens`)\n",
        "      query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
        "      # Shape of `context`: (`batch_size`, 1, `num_hiddens`)\n",
        "      context = self.attention(\n",
        "            query, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "      # Concatenate on the feature dimension\n",
        "      x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
        "      # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n",
        "      out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
        "      outputs.append(out)\n",
        "      self._attention_weights.append(self.attention.attention_weights)\n",
        "    # After fully-connected layer transformation, shape of `outputs`:\n",
        "    # (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    outputs = self.dense(torch.cat(outputs, dim=0))\n",
        "    return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
        "        enc_valid_lens]\n",
        "  @property\n",
        "  def attention_weights(self):\n",
        "    return self._attention_weights"
      ],
      "metadata": {
        "id": "GGIYLRK814SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
        "    num_layers=2)\n",
        "encoder.eval()\n",
        "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
        "    num_layers=2)\n",
        "decoder.eval()\n",
        "X = torch.zeros((4, 7), dtype=torch.long) # (`batch_size`, `num_steps`)\n",
        "state = decoder.init_state(encoder(X), None)\n",
        "output, state = decoder(X, state)\n",
        "output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
      ],
      "metadata": {
        "id": "pvNH0qws6yw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.4.3 Training"
      ],
      "metadata": {
        "id": "Dsns5QIP623v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs, device = 0.005, 250, d2l.try_gpu()\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = d2l.Seq2SeqEncoder(\n",
        "len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqAttentionDecoder(\n",
        "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
      ],
      "metadata": {
        "id": "LLUMTWgb63P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
        "for eng, fra in zip(engs, fras):\n",
        "  translation, dec_attention_weight_seq = d2l.predict_seq2seq(\n",
        "      net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
        "  print(f'{eng} => {translation}, ',\n",
        "        f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
      ],
      "metadata": {
        "id": "NeDYOzBd6_M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus one to include the end-of-sequence token\n",
        "d2l.show_heatmaps(\n",
        "    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\n",
        "    xlabel='Key positions', ylabel='Query positions')"
      ],
      "metadata": {
        "id": "Ail5vIL97H6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}